{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from pettingzoo.utils.env import ParallelEnv\n",
    "from gymnasium import spaces\n",
    "from ray.rllib.env import ParallelPettingZooEnv\n",
    "import numpy as np\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "import ray\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray import tune\n",
    "from ray.tune.registry import register_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrisonersDilemmaParallel(ParallelEnv):\n",
    "    def __init__(self):\n",
    "        self.possible_agents = [\"agent_0\", \"agent_1\"]\n",
    "        self.action_spaces = {agent: spaces.Discrete(2) for agent in self.possible_agents}\n",
    "        self.observation_spaces = {\n",
    "            agent: spaces.Box(low=0, high=1, shape=(2,), dtype=np.int8) for agent in self.possible_agents\n",
    "        }\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.agents = self.possible_agents[:]\n",
    "        self.dones = {agent: False for agent in self.possible_agents}\n",
    "        self.rewards = {agent: 0 for agent in self.possible_agents}\n",
    "        self.cumulative_reward = {agent : 0 for agent in self.possible_agents}\n",
    "        self.observations = {agent: np.array([0, 0]) for agent in self.possible_agents}\n",
    "        self.infos = {agent: {} for agent in self.possible_agents}\n",
    "        return self.observations\n",
    "\n",
    "    def step(self, actions):\n",
    "        if not all(agent in actions for agent in self.possible_agents):\n",
    "            raise ValueError(\"All agents must have an action\")\n",
    "\n",
    "        action_0, action_1 = actions[\"agent_0\"], actions[\"agent_1\"]\n",
    "\n",
    "        if action_0 == 0 and action_1 == 0:\n",
    "            rewards = [1, 1]\n",
    "        elif action_0 == 1 and action_1 == 0:\n",
    "            rewards = [10, 0]\n",
    "        elif action_0 == 0 and action_1 == 1:\n",
    "            rewards = [0, 10]\n",
    "        else:\n",
    "            rewards = [0, 0]\n",
    "\n",
    "        self.rewards = {\"agent_0\": rewards[0], \"agent_1\": rewards[1]}\n",
    "        for agent, cumul_reward in self.rewards.items():\n",
    "            self.cumulative_reward[agent] += self.rewards[agent]\n",
    "        self.dones = {\"agent_0\": False, \"agent_1\": False}\n",
    "        self.observations = {\n",
    "            \"agent_0\": np.array([actions[\"agent_0\"], actions[\"agent_1\"]]),\n",
    "            \"agent_1\": np.array([actions[\"agent_1\"], actions[\"agent_0\"]]),\n",
    "        }\n",
    "        return self.observations, self.rewards, self.dones, self.infos\n",
    "\n",
    "    def render(self):\n",
    "        print(f\"Agent 0: {self.observations['agent_0']}, Turn Reward: {self.rewards['agent_0']}, Cumul Reward : {self.cumulative_reward['agent_0']}\")\n",
    "        print(f\"Agent 1: {self.observations['agent_1']}, Turn Reward: {self.rewards['agent_1']}, Cumul Reward : {self.cumulative_reward['agent_1']}\")\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def observation_space(self, agent):\n",
    "        return self.observation_spaces[agent]\n",
    "\n",
    "    def action_space(self, agent):\n",
    "        return self.action_spaces[agent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_qq_tours(nb_tours: int = 5):\n",
    "    # Initialize the environment\n",
    "    env = PrisonersDilemmaParallel()\n",
    "\n",
    "    # Run 5 turns of the game\n",
    "    env.reset()\n",
    "    for turn in range(5):\n",
    "        actions = {\n",
    "            \"agent_0\": np.random.choice([0, 1]),  # Random action for agent 0\n",
    "            \"agent_1\": np.random.choice([0, 1])   # Random action for agent 1\n",
    "        }\n",
    "        observations, rewards, dones, infos = env.step(actions)\n",
    "        print(f\"Turn {turn + 1}:\")\n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe encapsulant l'environnement parallèle dans un environnement multi-agent\n",
    "class RLlibPrisonersDilemma(MultiAgentEnv):\n",
    "    def __init__(self):\n",
    "        self.env = PrisonersDilemmaParallel()\n",
    "        self.agents = self.env.possible_agents\n",
    "\n",
    "    def reset(self):\n",
    "        observations = self.env.reset()\n",
    "        return {agent: observations[agent] for agent in self.agents}\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        observations, rewards, dones, infos = self.env.step(action_dict)\n",
    "        return (\n",
    "            {agent: observations[agent] for agent in self.agents},\n",
    "            {agent: rewards[agent] for agent in self.agents},\n",
    "            {agent: dones[agent] for agent in self.agents},\n",
    "            {agent: infos[agent] for agent in self.agents},\n",
    "        )\n",
    "\n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n",
    "    def observation_space(self, agent):\n",
    "        return self.env.observation_space(agent)\n",
    "\n",
    "    def action_space(self, agent):\n",
    "        return self.env.action_space(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 11:25:01,222\tINFO worker.py:1582 -- Calling ray.init() again after it has already been called.\n",
      "2024-06-06 11:25:01,223\tINFO tune.py:614 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-06-06 11:25:08</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:07.49        </td></tr>\n",
       "<tr><td>Memory:      </td><td>17.6/31.7 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 2.0/20 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T600)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                                                                          </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_prisoners_dilemma_parallel_a65be_00000</td><td style=\"text-align: right;\">           1</td><td>C:/Users/P23B6~1.ARC/AppData/Local/Temp/ray/session_2024-06-06_11-20-03_814344_528/artifacts/2024-06-06_11-25-01/PPO_2024-06-06_11-25-01/driver_artifacts/PPO_prisoners_dilemma_parallel_a65be_00000_0_2024-06-06_11-25-01/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                </th><th>status  </th><th>loc            </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_prisoners_dilemma_parallel_a65be_00000</td><td>ERROR   </td><td>127.0.0.1:22864</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 11:25:08,703\tERROR tune_controller.py:1331 -- Trial task failed for trial PPO_prisoners_dilemma_parallel_a65be_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\air\\execution\\_internal\\event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\_private\\worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\_private\\worker.py\", line 861, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::PPO.train()\u001b[39m (pid=22864, ip=127.0.0.1, actor_id=818b56035f516ee3487d8d9201000000, repr=PPO)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1887, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1828, in ray._raylet.execute_task.function_executor\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\_private\\function_manager.py\", line 691, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\tune\\trainable\\trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\tune\\trainable\\trainable.py\", line 328, in train\n",
      "    result = self.step()\n",
      "             ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 878, in step\n",
      "    train_results, train_iter_ctx = self._run_one_training_iteration()\n",
      "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 3156, in _run_one_training_iteration\n",
      "    results = self.training_step()\n",
      "              ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\algorithms\\ppo\\ppo.py\", line 426, in training_step\n",
      "    return self._training_step_old_and_hybrid_api_stacks()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\algorithms\\ppo\\ppo.py\", line 576, in _training_step_old_and_hybrid_api_stacks\n",
      "    train_batch = synchronous_parallel_sample(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\execution\\rollout_ops.py\", line 97, in synchronous_parallel_sample\n",
      "    sampled_data = worker_set.foreach_worker(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 831, in foreach_worker\n",
      "    handle_remote_call_result_errors(\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 80, in handle_remote_call_result_errors\n",
      "    raise r.get()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=28680, ip=127.0.0.1, actor_id=ca2fb9fe7819fcf6bd3be4fe01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000029F0D20F110>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1887, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1828, in ray._raylet.execute_task.function_executor\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\_private\\function_manager.py\", line 691, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\", line 189, in apply\n",
      "    raise e\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\", line 178, in apply\n",
      "    return func(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\execution\\rollout_ops.py\", line 99, in <lambda>\n",
      "    (lambda w: w.sample())\n",
      "               ^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 685, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 91, in next\n",
      "    batches = [self.get_data()]\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 273, in get_data\n",
      "    item = next(self._env_runner)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\evaluation\\env_runner_v2.py\", line 348, in run\n",
      "    outputs = self.step()\n",
      "              ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\evaluation\\env_runner_v2.py\", line 367, in step\n",
      "    ) = self._base_env.poll()\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\env\\multi_agent_env.py\", line 624, in poll\n",
      "    ) = env_state.poll()\n",
      "        ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\env\\multi_agent_env.py\", line 827, in poll\n",
      "    self.reset()\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\env\\multi_agent_env.py\", line 911, in reset\n",
      "    raise e\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\env\\multi_agent_env.py\", line 905, in reset\n",
      "    obs_and_infos = self.env.reset(seed=seed, options=options)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: RLlibPrisonersDilemma.reset() got an unexpected keyword argument 'seed'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_prisoners_dilemma_parallel_a65be_00000</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 11:25:08,714\tINFO tune.py:1007 -- Wrote the latest version of all result files and experiment state to 'C:/Users/p.archipczuk/ray_results/PPO_2024-06-06_11-25-01' in 0.0059s.\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [PPO_prisoners_dilemma_parallel_a65be_00000])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m ray\u001b[38;5;241m.\u001b[39minit(ignore_reinit_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Lancement de l'entraînement\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m tune\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPPO\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     29\u001b[0m     stop\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisodes_total\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5000\u001b[39m},\n\u001b[0;32m     30\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig\n\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Arrêt de Ray\u001b[39;00m\n\u001b[0;32m     34\u001b[0m ray\u001b[38;5;241m.\u001b[39mshutdown()\n",
      "File \u001b[1;32mc:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\tune\\tune.py:1033\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, storage_filesystem, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, resume, resume_config, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, chdir_to_trial_dir, local_dir, _remote, _remote_string_queue, _entrypoint)\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m incomplete_trials:\n\u001b[0;32m   1032\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raise_on_failed_trial \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m experiment_interrupted_event\u001b[38;5;241m.\u001b[39mis_set():\n\u001b[1;32m-> 1033\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TuneError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrials did not complete\u001b[39m\u001b[38;5;124m\"\u001b[39m, incomplete_trials)\n\u001b[0;32m   1034\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1035\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrials did not complete: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, incomplete_trials)\n",
      "\u001b[1;31mTuneError\u001b[0m: ('Trials did not complete', [PPO_prisoners_dilemma_parallel_a65be_00000])"
     ]
    }
   ],
   "source": [
    "# Création de l'environnement\n",
    "def env_creator(_):\n",
    "    return RLlibPrisonersDilemma()\n",
    "\n",
    "# Enregistrement de l'environnement\n",
    "register_env(\"prisoners_dilemma_parallel\", env_creator)\n",
    "\n",
    "# Configuration de l'entraînement\n",
    "config = {\n",
    "    \"env\": \"prisoners_dilemma_parallel\",\n",
    "    \"framework\": \"torch\",  # or \"tf\"\n",
    "    \"num_gpus\": 0,\n",
    "    \"num_workers\": 1,\n",
    "    \"multiagent\": {\n",
    "        \"policies\": {\n",
    "            \"policy_0\": (None, spaces.Box(low=0, high=1, shape=(2,), dtype=np.int8), spaces.Discrete(2), {}),\n",
    "            \"policy_1\": (None, spaces.Box(low=0, high=1, shape=(2,), dtype=np.int8), spaces.Discrete(2), {}),\n",
    "        },\n",
    "        \"policy_mapping_fn\": lambda agent_id: \"policy_0\" if agent_id == \"agent_0\" else \"policy_1\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Initialisation de Ray\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# Lancement de l'entraînement\n",
    "tune.run(\n",
    "    \"PPO\",\n",
    "    stop={\"episodes_total\": 5000},\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Arrêt de Ray\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 11:26:34,988\tINFO worker.py:1582 -- Calling ray.init() again after it has already been called.\n",
      "2024-06-06 11:26:34,989\tINFO tune.py:614 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-06-06 11:26:42</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:07.40        </td></tr>\n",
       "<tr><td>Memory:      </td><td>17.7/31.7 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 2.0/20 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T600)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                                                                          </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_prisoners_dilemma_parallel_de3f4_00000</td><td style=\"text-align: right;\">           1</td><td>C:/Users/P23B6~1.ARC/AppData/Local/Temp/ray/session_2024-06-06_11-20-03_814344_528/artifacts/2024-06-06_11-26-34/PPO_2024-06-06_11-26-34/driver_artifacts/PPO_prisoners_dilemma_parallel_de3f4_00000_0_2024-06-06_11-26-34/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                </th><th>status  </th><th>loc           </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_prisoners_dilemma_parallel_de3f4_00000</td><td>ERROR   </td><td>127.0.0.1:7960</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 11:26:42,380\tERROR tune_controller.py:1331 -- Trial task failed for trial PPO_prisoners_dilemma_parallel_de3f4_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\air\\execution\\_internal\\event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\_private\\worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\_private\\worker.py\", line 861, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::PPO.train()\u001b[39m (pid=7960, ip=127.0.0.1, actor_id=650a707cf3560709f5489c4c01000000, repr=PPO)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1887, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1828, in ray._raylet.execute_task.function_executor\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\_private\\function_manager.py\", line 691, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\tune\\trainable\\trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\tune\\trainable\\trainable.py\", line 328, in train\n",
      "    result = self.step()\n",
      "             ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 878, in step\n",
      "    train_results, train_iter_ctx = self._run_one_training_iteration()\n",
      "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 3156, in _run_one_training_iteration\n",
      "    results = self.training_step()\n",
      "              ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\algorithms\\ppo\\ppo.py\", line 426, in training_step\n",
      "    return self._training_step_old_and_hybrid_api_stacks()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\algorithms\\ppo\\ppo.py\", line 576, in _training_step_old_and_hybrid_api_stacks\n",
      "    train_batch = synchronous_parallel_sample(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\execution\\rollout_ops.py\", line 97, in synchronous_parallel_sample\n",
      "    sampled_data = worker_set.foreach_worker(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 831, in foreach_worker\n",
      "    handle_remote_call_result_errors(\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 80, in handle_remote_call_result_errors\n",
      "    raise r.get()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=5532, ip=127.0.0.1, actor_id=cd006ebfe9c0f754f105e99a01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000022C8CEEE310>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1887, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1828, in ray._raylet.execute_task.function_executor\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\_private\\function_manager.py\", line 691, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\", line 189, in apply\n",
      "    raise e\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\", line 178, in apply\n",
      "    return func(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\execution\\rollout_ops.py\", line 99, in <lambda>\n",
      "    (lambda w: w.sample())\n",
      "               ^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 685, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 91, in next\n",
      "    batches = [self.get_data()]\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 273, in get_data\n",
      "    item = next(self._env_runner)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\evaluation\\env_runner_v2.py\", line 348, in run\n",
      "    outputs = self.step()\n",
      "              ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\evaluation\\env_runner_v2.py\", line 367, in step\n",
      "    ) = self._base_env.poll()\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\env\\multi_agent_env.py\", line 624, in poll\n",
      "    ) = env_state.poll()\n",
      "        ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\rllib\\env\\multi_agent_env.py\", line 857, in poll\n",
      "    for ag in observations.keys():\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'str' object has no attribute 'keys'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_prisoners_dilemma_parallel_de3f4_00000</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 11:26:42,389\tINFO tune.py:1007 -- Wrote the latest version of all result files and experiment state to 'C:/Users/p.archipczuk/ray_results/PPO_2024-06-06_11-26-34' in 0.0050s.\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [PPO_prisoners_dilemma_parallel_de3f4_00000])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 117\u001b[0m\n\u001b[0;32m    114\u001b[0m ray\u001b[38;5;241m.\u001b[39minit(ignore_reinit_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Lancement de l'entraînement\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m tune\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPPO\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    119\u001b[0m     stop\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisodes_total\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5000\u001b[39m},\n\u001b[0;32m    120\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig\n\u001b[0;32m    121\u001b[0m )\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Arrêt de Ray\u001b[39;00m\n\u001b[0;32m    124\u001b[0m ray\u001b[38;5;241m.\u001b[39mshutdown()\n",
      "File \u001b[1;32mc:\\Users\\p.archipczuk\\AppData\\Local\\anaconda3\\envs\\RL\\Lib\\site-packages\\ray\\tune\\tune.py:1033\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, storage_filesystem, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, resume, resume_config, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, chdir_to_trial_dir, local_dir, _remote, _remote_string_queue, _entrypoint)\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m incomplete_trials:\n\u001b[0;32m   1032\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raise_on_failed_trial \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m experiment_interrupted_event\u001b[38;5;241m.\u001b[39mis_set():\n\u001b[1;32m-> 1033\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TuneError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrials did not complete\u001b[39m\u001b[38;5;124m\"\u001b[39m, incomplete_trials)\n\u001b[0;32m   1034\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1035\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrials did not complete: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, incomplete_trials)\n",
      "\u001b[1;31mTuneError\u001b[0m: ('Trials did not complete', [PPO_prisoners_dilemma_parallel_de3f4_00000])"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from pettingzoo.utils.env import ParallelEnv\n",
    "\n",
    "# Environnement du Dilemme du Prisonnier\n",
    "class PrisonersDilemmaParallel(ParallelEnv):\n",
    "    def __init__(self):\n",
    "        self.possible_agents = [\"agent_0\", \"agent_1\"]\n",
    "        self.action_spaces = {agent: spaces.Discrete(2) for agent in self.possible_agents}\n",
    "        self.observation_spaces = {\n",
    "            agent: spaces.Box(low=0, high=1, shape=(2,), dtype=np.int8) for agent in self.possible_agents\n",
    "        }\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.agents = self.possible_agents[:]\n",
    "        self.dones = {agent: False for agent in self.possible_agents}\n",
    "        self.rewards = {agent: 0 for agent in self.possible_agents}\n",
    "        self.observations = {agent: np.array([0, 0]) for agent in self.possible_agents}\n",
    "        self.infos = {agent: {} for agent in self.possible_agents}\n",
    "        return self.observations\n",
    "\n",
    "    def step(self, actions):\n",
    "        if not all(agent in actions for agent in self.possible_agents):\n",
    "            raise ValueError(\"All agents must have an action\")\n",
    "\n",
    "        action_0, action_1 = actions[\"agent_0\"], actions[\"agent_1\"]\n",
    "\n",
    "        if action_0 == 0 and action_1 == 0:\n",
    "            rewards = [100, 100]\n",
    "        elif action_0 == 1 and action_1 == 0:\n",
    "            rewards = [200, 0]\n",
    "        elif action_0 == 0 and action_1 == 1:\n",
    "            rewards = [0, 200]\n",
    "        else:\n",
    "            rewards = [0, 0]\n",
    "\n",
    "        self.rewards = {\"agent_0\": rewards[0], \"agent_1\": rewards[1]}\n",
    "        self.dones = {\"agent_0\": False, \"agent_1\": False}\n",
    "        self.observations = {\n",
    "            \"agent_0\": np.array([actions[\"agent_0\"], actions[\"agent_1\"]]),\n",
    "            \"agent_1\": np.array([actions[\"agent_1\"], actions[\"agent_0\"]]),\n",
    "        }\n",
    "        return self.observations, self.rewards, self.dones, self.infos\n",
    "\n",
    "    def render(self):\n",
    "        print(f\"Agent 0: {self.observations['agent_0']}, Reward: {self.rewards['agent_0']}\")\n",
    "        print(f\"Agent 1: {self.observations['agent_1']}, Reward: {self.rewards['agent_1']}\")\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def observation_space(self, agent):\n",
    "        return self.observation_spaces[agent]\n",
    "\n",
    "    def action_space(self, agent):\n",
    "        return self.action_spaces[agent]\n",
    "\n",
    "# Classe encapsulant l'environnement parallèle dans un environnement multi-agent\n",
    "class RLlibPrisonersDilemma(MultiAgentEnv):\n",
    "    def __init__(self):\n",
    "        self.env = PrisonersDilemmaParallel()\n",
    "        self.agents = self.env.possible_agents\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        observations = self.env.reset(seed=seed)\n",
    "        return {agent: observations[agent] for agent in self.agents}\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        observations, rewards, dones, infos = self.env.step(action_dict)\n",
    "        return (\n",
    "            {agent: observations[agent] for agent in self.agents},\n",
    "            {agent: rewards[agent] for agent in self.agents},\n",
    "            {agent: dones[agent] for agent in self.agents},\n",
    "            {agent: infos[agent] for agent in self.agents},\n",
    "        )\n",
    "\n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n",
    "    def observation_space(self, agent):\n",
    "        return self.env.observation_space(agent)\n",
    "\n",
    "    def action_space(self, agent):\n",
    "        return self.env.action_space(agent)\n",
    "\n",
    "# Création de l'environnement\n",
    "def env_creator(_):\n",
    "    return RLlibPrisonersDilemma()\n",
    "\n",
    "# Enregistrement de l'environnement\n",
    "register_env(\"prisoners_dilemma_parallel\", env_creator)\n",
    "\n",
    "# Configuration de l'entraînement\n",
    "config = {\n",
    "    \"env\": \"prisoners_dilemma_parallel\",\n",
    "    \"framework\": \"torch\",  # or \"tf\"\n",
    "    \"num_gpus\": 0,\n",
    "    \"num_workers\": 1,\n",
    "    \"multiagent\": {\n",
    "        \"policies\": {\n",
    "            \"policy_0\": (None, spaces.Box(low=0, high=1, shape=(2,), dtype=np.int8), spaces.Discrete(2), {}),\n",
    "            \"policy_1\": (None, spaces.Box(low=0, high=1, shape=(2,), dtype=np.int8), spaces.Discrete(2), {}),\n",
    "        },\n",
    "        \"policy_mapping_fn\": lambda agent_id: \"policy_0\" if agent_id == \"agent_0\" else \"policy_1\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Initialisation de Ray\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# Lancement de l'entraînement\n",
    "tune.run(\n",
    "    \"PPO\",\n",
    "    stop={\"episodes_total\": 5000},\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Arrêt de Ray\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
